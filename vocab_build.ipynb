{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import digits\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "folder_path=Path(\"Flickr8k/captions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences,threshold):\n",
    "    new_descrip=[]\n",
    "    counter=Counter()\n",
    "    for line in sentences:\n",
    "        #tokenize sentence into words\n",
    "        token=line.split()\n",
    "        #lowercase words in list\n",
    "        low_word=[word.lower() for word in token]\n",
    "        #remove digits from the list\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        word_list= [i.translate(remove_digits) for i in low_word]\n",
    "        #remove empty string elements from the list\n",
    "        remove_space=list(filter(None,word_list))\n",
    "        #update values in counter\n",
    "        counter.update(remove_space)\n",
    "        #make new list of cleaned descriptions\n",
    "        new_descrip.append(\" \".join(remove_space))\n",
    "    #remove words from vocabulary if frequency is less than given threshold    \n",
    "    vocabulary=[key for key,value in counter.items() if value>=threshold]    \n",
    "        \n",
    "    return vocabulary,new_descrip\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "pd_dataset=pd.read_csv(\"Flickr8k/captions/train_dataset.txt\",delimiter=\"\\t\")\n",
    "pd_frame=pd_dataset.values\n",
    "print(pd_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[]\n",
    "for sent in range(pd_frame.shape[0]):\n",
    "    sentence=pd_frame[sent][1]\n",
    "    sentences.append(sentence)#append the sentence in list \"sentences\"\n",
    "len(sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary,clean_desc=build_vocab(sentences,1)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization\n",
    "word2id={value:index for index,value in enumerate(vocabulary)}\n",
    "id2word={index:value for index,value in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id[id2word.get(0)]=7668\n",
    "word2id[\"PAD\"]=0\n",
    "\n",
    "id2word[7668]=id2word.get(0)\n",
    "id2word[0]=\"PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7669"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(id2word.keys())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "#find description with maximum length\n",
    "max_len=0\n",
    "for i in clean_desc:\n",
    "    sent=i.split()\n",
    "    if len(sent)>max_len:\n",
    "        max_len=len(sent)\n",
    "print(max_len)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b32693543623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpadded_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_pad_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msubseq_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextwords_hotvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpadded_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "padded_seq,subseq_seq=[],[]\n",
    "for line in range(len(clean_desc)):\n",
    "    partial_seq=[]\n",
    "    next_words=[]\n",
    "    text=clean_desc[line].split()\n",
    "    encod_text=[word2id[i] for i in text]#encode string to vectors\n",
    "    for i in range(1,len(encod_text)):\n",
    "        partial_seq.append(encod_text[:i])#list of lists of partial sentences in vector form\n",
    "        next_words.append(encod_text[i])#store next word or target word in the list\n",
    "    #post(to the right) padding sequences with zeros to fixed(maximum) length    \n",
    "    partial_pad_seq=sequence.pad_sequences(partial_seq,max_len,padding=\"post\")\n",
    "    nextwords_hotvec = np.zeros([len(next_words), vocab_size], dtype=np.bool)#numpy array of falses \n",
    "    #vectorization\n",
    "    for i,next_word in enumerate(next_words):\n",
    "        nextwords_hotvec[i,next_word]=1\n",
    "    padded_seq.append(partial_pad_seq)\n",
    "    subseq_seq.append(nextwords_hotvec)\n",
    "padded_seq=np.asarray(padded_seq)\n",
    "subseq_seq=np.asarray(subseq_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 5), dtype=float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = np.zeros([0, max_len])#squeeze rows to zero(rows are not fixed)\n",
    "next_words = np.zeros([0, vocab_size])\n",
    "num_of_images=6000\n",
    "for ix in range(num_of_images):\n",
    "    #concatenate captions and next words of all the images\n",
    "    captions = np.concatenate([captions, padded_sequences[ix]])\n",
    "    next_words = np.concatenate([next_words, subsequent_words[ix]])\n",
    "\n",
    "    \n",
    "np.save(folder_path/captions.npy, captions)#save numpy array in npy file\n",
    "np.save(folder_path/next_words.npy, next_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
